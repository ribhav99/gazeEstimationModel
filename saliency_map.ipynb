{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Fixed Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pickle as pkl\n",
    "import os\n",
    "import json\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"data\"\n",
    "output_folder = \"output\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to compute first derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dx_dt(x: np.array, dt: float = 1, method=1):\n",
    "    \"\"\"\n",
    "    This function compute first derivative for the input function x using either central or forward differences\n",
    "\n",
    "    :param x: input array to compute derivative, should be of shape [num of timestamp, num of attributes]\n",
    "    :param dt: time stamp size\n",
    "    :param method: method of computing derivative. 1 is forward difference, 2 is central differences\n",
    "    :return: dx/dt, would be the same size as x. The first and last element are zero.\n",
    "    \"\"\"\n",
    "    out_dx_dt = np.zeros(x.shape)\n",
    "    if len(x.shape) == 2:\n",
    "        for j in range(0, x.shape[1]):\n",
    "            if method == 1:\n",
    "                for i in range(0, x.shape[0] - 1):\n",
    "                    out_dx_dt[i, j] = (x[i + 1, j] - x[i, j])/dt\n",
    "                out_dx_dt[-1, j] = out_dx_dt[-2, j]\n",
    "            if method == 2:\n",
    "                for i in range(1, x.shape[0] - 1):\n",
    "                    out_dx_dt[i, j] = (x[i + 1, j] - x[i - 1, j]) / 2 / dt\n",
    "                out_dx_dt[-1, j] = out_dx_dt[-2, j]\n",
    "                out_dx_dt[0, j] = out_dx_dt[1, j]\n",
    "    elif len(x.shape) == 1:\n",
    "        if method == 1:\n",
    "            for i in range(0, x.shape[0] - 1):\n",
    "                out_dx_dt[i] = (x[i + 1] - x[i]) / dt\n",
    "            out_dx_dt[-1] = 0\n",
    "        if method == 2:\n",
    "            for i in range(1, x.shape[0] - 1):\n",
    "                out_dx_dt[i] = (x[i + 1] - x[i - 1]) / 2 / dt\n",
    "            out_dx_dt[-1] = 0\n",
    "            out_dx_dt[0] = 0\n",
    "    return out_dx_dt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to compute gaze centers and intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispersion_filtering(x, y, dispersion_threshold = 6, duraiton_threshold=0.3):\n",
    "    start = 0\n",
    "    window = []\n",
    "    fps = int(np.round(1/(x[1] - x[0])))\n",
    "    duration_threshold_frames = np.ceil(duraiton_threshold * fps)\n",
    "    \n",
    "    def dispersion(arr):\n",
    "        # input is a 2d array\n",
    "        disp = np.max(arr[:, 0]) - np.min(arr[:, 0]) + np.max(arr[:, 1]) - np.min(arr[:, 1])\n",
    "        return disp\n",
    "\n",
    "    fixations = []\n",
    "    fixations_intervals = []\n",
    "    \n",
    "    # while there are still points\n",
    "    while int(start+duration_threshold_frames) < y.shape[0]:\n",
    "        # initialize a window:\n",
    "        window = list(range(int(start), int(start+duration_threshold_frames)))\n",
    "        start = start + duration_threshold_frames\n",
    "        disp = dispersion(y[window])\n",
    "        while disp <= dispersion_threshold:\n",
    "            disp = dispersion(y[window])\n",
    "            if window[-1]+1 < y.shape[0]:\n",
    "                window.append(window[-1]+1)\n",
    "            start = start + 1\n",
    "            if start >= y.shape[0]:\n",
    "                break\n",
    "        # if the current set of points never fit the duration criteria\n",
    "        if len(window) <= duration_threshold_frames:\n",
    "            start = start + 1\n",
    "        # otherwise note it as fixations\n",
    "        else:\n",
    "            centroid = np.mean(y[window], axis=0)\n",
    "            duration = (window[-1] - window[0]) / fps\n",
    "            fixations.append([centroid[0], centroid[1], duration])\n",
    "            fixations_intervals.append([window[0], window[-1]])\n",
    "    fixations = np.array(fixations)\n",
    "    return fixations, fixations_intervals\n",
    "\n",
    "def switch_rate_distance(points, center, radius):\n",
    "    dist = np.linalg.norm(points - center, axis=1)\n",
    "    in_cluster = np.where(dist < radius, 1, 0)\n",
    "    # intracluster distance\n",
    "    total_in = np.sum(in_cluster)\n",
    "    # switch rate (we want to maximize the number of gaze shifts between in cluster and out of cluster)\n",
    "    # the rationale behind this is that gaze shifting outside the cluster \n",
    "    swtich = np.sum(np.abs(dx_dt(in_cluster)))/total_in\n",
    "    rtv = -swtich\n",
    "    return rtv, total_in\n",
    "\n",
    "def intra_cluster_distance(points, center, radius):\n",
    "    dist = np.linalg.norm(points - center, axis=1)\n",
    "    in_cluster = np.where(dist < radius, 1, 0)\n",
    "    # intracluster distance\n",
    "    rtv = np.sum(dist * in_cluster)\n",
    "    total_in = np.sum(in_cluster)\n",
    "    rtv = rtv / total_in\n",
    "    return rtv, total_in\n",
    "\n",
    "def radius_line_search(points, center, min_member, max_iter=7):\n",
    "    center = np.expand_dims(center, axis=0)\n",
    "    radius_max = 20\n",
    "    radius_min = 1\n",
    "    for i in range(0, max_iter):\n",
    "        clustering_goodness_max_r, total_in= switch_rate_distance(points, center, radius_max)\n",
    "        clustering_goodness_min_r, total_in= switch_rate_distance(points, center, radius_min)\n",
    "        if clustering_goodness_max_r <= clustering_goodness_min_r and total_in >= min_member:\n",
    "            radius_max = (radius_max + radius_min)/2\n",
    "        else:\n",
    "            radius_min = (radius_max + radius_min)/2\n",
    "        \n",
    "    return radius_min   \n",
    "\n",
    "def find_gaze_target(fixations, gaze_points, vertical_sensitivity=1):\n",
    "\n",
    "    fixations[:, 1] = fixations[:, 1] * vertical_sensitivity\n",
    "    gaze_points[:, 1] = gaze_points[:, 1] * vertical_sensitivity\n",
    "    if fixations.shape[0] <= 2:\n",
    "        return np.ones((fixations.shape[0], ))\n",
    "\n",
    "    mixture = GaussianMixture(int(np.minimum(8, fixations.shape[0]))).fit(fixations[:, :2], )\n",
    "    mix = mixture.predict_proba(fixations[:, :2])\n",
    "    # get cluster heads\n",
    "    mixture_centers = mixture.means_\n",
    "    # find the index of the most likely target\n",
    "    most_likely_target = np.argmax(np.sum(mix, axis=0))\n",
    "    most_likely_cluster_member_count = np.where(np.argmax(mix, axis=1) == most_likely_target, 1, 0).sum()\n",
    "    # find the most likely look at point\n",
    "    gaze_target = mixture_centers[most_likely_target]\n",
    "    distance_to_target = np.linalg.norm(gaze_points[:, :2] - np.expand_dims(gaze_target, axis=0), axis=1)\n",
    "    # use line search to find an appropriete radious of what to include\n",
    "    radius = radius_line_search(fixations[:, :2], gaze_target, most_likely_cluster_member_count, 7)\n",
    "    return gaze_target, radius"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to increment image values in a gaussian style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_pixel_gaussian(image, center, sigma):\n",
    "    # Create a 2D Gaussian kernel\n",
    "    kernel_size = int(2 * np.ceil(2 * sigma) + 1)\n",
    "    kernel = np.zeros((kernel_size, kernel_size))\n",
    "    center_index = int(kernel_size / 2)\n",
    "    for i in range(kernel_size):\n",
    "        for j in range(kernel_size):\n",
    "            x = i - center_index\n",
    "            y = j - center_index\n",
    "            kernel[i, j] = np.exp(-(x ** 2 + y ** 2) / (2 * sigma ** 2))\n",
    "    kernel /= np.sum(kernel)\n",
    "    \n",
    "    # Increment the pixel values according to the Gaussian kernel\n",
    "    row, col = int(center[0]), int(center[1])\n",
    "    row_min = int(max(row - center_index, 0))\n",
    "    row_max = int(min(row + center_index + 1, image.shape[0]))\n",
    "    col_min = int(max(col - center_index, 0))\n",
    "    col_max = int(min(col + center_index + 1, image.shape[1]))\n",
    "    window = image[row_min:row_max, col_min:col_max]\n",
    "    weights = kernel[center_index - (row - row_min):center_index + (row_max - row), \n",
    "                     center_index - (col - col_min):center_index + (col_max - col)]\n",
    "    window += weights\n",
    "    image[row_min:row_max, col_min:col_max] = window\n",
    "    return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get static saliency map for each shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names_path = os.path.join(*[output_folder, \"metadata.json\"])\n",
    "with open(video_names_path, \"r\") as f:\n",
    "    video_metadata = json.load(f)[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take in angle between 0 and 360 and return angle between -180 and 180 and shift it by 180\n",
    "def convert_and_shift_angles(x):\n",
    "    if x > 180:\n",
    "        x -=  360\n",
    "    return x + 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/3900 [00:08<26:36,  2.43it/s]"
     ]
    }
   ],
   "source": [
    "canvas_size = (360, 360)\n",
    "canvas_size = (int(np.ceil(canvas_size[0])), int(np.ceil(canvas_size[1])))\n",
    "for shot_id in trange(0, len(os.listdir(os.path.join('output', 'gaze')))):\n",
    "    gaze = np.load(os.path.join(*[output_folder, \"gaze\", f'clip_{shot_id}.npy']))\n",
    "    ts = np.arange(0, gaze.shape[0]) / 25\n",
    "    fixations, fixations_intervals = dispersion_filtering(ts, gaze, dispersion_threshold=6, duraiton_threshold=0.2)\n",
    "    image = np.zeros((int(np.ceil(canvas_size[0])), int(np.ceil(canvas_size[1]))))\n",
    "    if len(fixations.shape) < 2:\n",
    "        np.save(os.path.join(output_folder, \"saliency_map\", f\"clip_{shot_id}.npy\"), image)\n",
    "        np.save(os.path.join(output_folder, \"fixation\", f\"clip_{shot_id}.npy\"), fixations)\n",
    "        continue\n",
    "\n",
    "    fixation_t = np.zeros(gaze.shape)\n",
    "    for i in range(0, len(fixations_intervals)):\n",
    "        for k in range(fixations_intervals[i][0], fixations_intervals[i][1]+1):\n",
    "            fixation_t[k] = fixations[i, :2]\n",
    "        if i == 0:\n",
    "            for k in range(0, fixations_intervals[i][1]):\n",
    "                fixation_t[k] = fixations[i, :2]\n",
    "        else:\n",
    "            for k in range(fixations_intervals[i-1][1], fixations_intervals[0][0]):\n",
    "                fixation_t[k] = fixations[i, :2]\n",
    "        if i == (len(fixations_intervals) - 1):\n",
    "            for k in range(fixations_intervals[i][1], len(fixations_intervals)):\n",
    "                fixation_t[k] = fixations[i, :2]\n",
    "                \n",
    "    fixation_t = np.vectorize(convert_and_shift_angles)(fixation_t)\n",
    "\n",
    "    for point in fixation_t:\n",
    "        point_x = int(point[0])\n",
    "        point_y = int(point[1])\n",
    "        image = increment_pixel_gaussian(image, point, 10)\n",
    "    # save image and fixation as .npy\n",
    "    np.save(os.path.join(output_folder, \"saliency_map\", f\"clip_{shot_id}.npy\"), image)\n",
    "    np.save(os.path.join(output_folder, \"fixation\", f\"clip_{shot_id}.npy\"), fixation_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.load(os.path.join(output_folder, \"saliency_map\", \"clip_0.npy\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyMapAudio(Dataset):\n",
    "    def __init__(self, processed_data_path):\n",
    "        self.filler = np.array([-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715])\n",
    "        # save dataset root path\n",
    "        self.data_root_path = processed_data_path\n",
    "        # load video names\n",
    "        video_names_path = os.path.join(*[self.data_root_path, \"video_to_window_metadata.json\"])\n",
    "        self.metadata = json.load(open(video_names_path, \"r\"))\n",
    "        self.all_files_in_set = []\n",
    "        videos_included = list(self.metadata.keys())\n",
    "        for i in videos_included:\n",
    "            self.all_files_in_set = self.all_files_in_set + self.metadata[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files_in_set)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        onscreen_audio_feature_path = os.path.join(*[self.data_root_path, \"audio\", \"clip_{}_speaker_{}.npy\".format(idx, 0)])\n",
    "        offscreen_audio_feature_path = os.path.join(*[self.data_root_path, \"audio\", \"clip_{}_speaker_{}.npy\".format(idx, 1)])\n",
    "        onscreen_text_feature_path = os.path.join(*[self.data_root_path, \"text\", \"clip_{}_speaker_{}.npy\".format(idx, 0)])\n",
    "        offscreen_text_feature_path = os.path.join(*[self.data_root_path, \"text\", \"clip_{}_speaker_{}.npy\".format(idx, 1)])\n",
    "        \n",
    "        input_audio_on_screen = np.load(onscreen_audio_feature_path)\n",
    "        input_audio_off_screen = np.load(offscreen_audio_feature_path)\n",
    "        \n",
    "        input_text_on_screen = np.load(onscreen_text_feature_path)\n",
    "        input_text_off_screen = np.load(offscreen_text_feature_path)\n",
    "            \n",
    "        if input_audio_on_screen.shape[0] < input_text_on_screen.shape[0]:\n",
    "            missing_frames = input_text_on_screen.shape[0] - input_audio_on_screen.shape[0]\n",
    "            padding = np.tile(np.expand_dims(self.filler, axis=0), [missing_frames, 1])\n",
    "            input_audio_on_screen = np.concatenate([input_audio_on_screen, padding], axis=0)\n",
    "            input_audio_off_screen = np.concatenate([input_audio_off_screen, padding], axis=0)\n",
    "        input_vector_onscreen = np.concatenate([input_audio_on_screen, input_text_on_screen], axis=1)\n",
    "        input_vector_offscreen = np.concatenate([input_audio_off_screen, input_text_off_screen], axis=1)\n",
    "        input_vector = np.concatenate([input_vector_onscreen, input_vector_offscreen], axis=1)\n",
    "\n",
    "        saliency_map = np.load(os.path.join(self.data_root_path, \"saliency_map\", f\"clip_{idx}.npy\"))\n",
    "\n",
    "        fixations = np.load(os.path.join(self.data_root_path, \"fixation\", f\"clip_{idx}.npy\"))\n",
    "        if len(fixations.shape) == 2:\n",
    "            targets = []\n",
    "            for i in range(fixations.shape[0]):\n",
    "                cur_target = np.zeros(saliency_map.shape)\n",
    "                fixation_center = fixations[i]\n",
    "                image_to_add = increment_pixel_gaussian(cur_target, fixation_center, 10)\n",
    "                assert cur_target.shape == image_to_add.shape\n",
    "                targets.append(image_to_add)\n",
    "        else:\n",
    "            targets = [np.zeros(saliency_map.shape) for _ in range(input_vector.shape[0])]\n",
    "\n",
    "        return input_vector, saliency_map, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in the dataset is a tuple of (input_vector, saliency_map, target)\n",
    "#   The input_vector is a concatenation of the onscreen and offscreen audio and text features\n",
    "#   It consists of 250 frames\n",
    "\n",
    "#   The saliency_map is an image with values between 0 and 1. It is the saliency map for the clip\n",
    "\n",
    "#   The target is a list of images with values between 0 and 1. It is the fixation map for each frame in the clip\n",
    "dataset = SaliencyMapAudio('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_x1_shape = dataset.__getitem__(0)[0].shape\n",
    "original_x2_shape = dataset.__getitem__(0)[1].shape\n",
    "original_y_shape = dataset.__getitem__(0)[2].shape\n",
    "for i in trange(dataset.__len__()):\n",
    "    x1, x2, y = dataset.__getitem__(i)\n",
    "    if x1.shape != original_x1_shape:\n",
    "        print('x1', x1.shape, original_x1_shape)\n",
    "    if x2.shape != original_x2_shape:\n",
    "        print('x2', x2.shape, original_x2_shape)\n",
    "    if y.shape != original_y_shape:\n",
    "        print('y', y.shape, original_y_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Fixed Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'output'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_pixel_gaussian(image, center, sigma):\n",
    "    # Create a 2D Gaussian kernel\n",
    "    kernel_size = int(2 * np.ceil(2 * sigma) + 1)\n",
    "    kernel = np.zeros((kernel_size, kernel_size))\n",
    "    center_index = int(kernel_size / 2)\n",
    "    for i in range(kernel_size):\n",
    "        for j in range(kernel_size):\n",
    "            x = i - center_index\n",
    "            y = j - center_index\n",
    "            kernel[i, j] = np.exp(-(x ** 2 + y ** 2) / (2 * sigma ** 2))\n",
    "    kernel /= np.sum(kernel)\n",
    "    \n",
    "    # Increment the pixel values according to the Gaussian kernel\n",
    "    row, col = int(center[0]), int(center[1])\n",
    "    row_min = int(max(row - center_index, 0))\n",
    "    row_max = int(min(row + center_index + 1, image.shape[0]))\n",
    "    col_min = int(max(col - center_index, 0))\n",
    "    col_max = int(min(col + center_index + 1, image.shape[1]))\n",
    "    window = image[row_min:row_max, col_min:col_max]\n",
    "    weights = kernel[center_index - (row - row_min):center_index + (row_max - row), \n",
    "                     center_index - (col - col_min):center_index + (col_max - col)]\n",
    "    window += weights\n",
    "    image[row_min:row_max, col_min:col_max] = window\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyMapAudio(Dataset):\n",
    "    def __init__(self, processed_data_path):\n",
    "        self.filler = np.array([-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715])\n",
    "        # save dataset root path\n",
    "        self.data_root_path = processed_data_path\n",
    "        # load video names\n",
    "        video_names_path = os.path.join(*[self.data_root_path, \"video_to_window_metadata.json\"])\n",
    "        self.metadata = json.load(open(video_names_path, \"r\"))\n",
    "        self.all_files_in_set = []\n",
    "        videos_included = list(self.metadata.keys())\n",
    "        for i in videos_included:\n",
    "            self.all_files_in_set = self.all_files_in_set + self.metadata[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files_in_set)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        onscreen_audio_feature_path = os.path.join(*[self.data_root_path, \"audio\", \"clip_{}_speaker_{}.npy\".format(idx, 0)])\n",
    "        offscreen_audio_feature_path = os.path.join(*[self.data_root_path, \"audio\", \"clip_{}_speaker_{}.npy\".format(idx, 1)])\n",
    "        onscreen_text_feature_path = os.path.join(*[self.data_root_path, \"text\", \"clip_{}_speaker_{}.npy\".format(idx, 0)])\n",
    "        offscreen_text_feature_path = os.path.join(*[self.data_root_path, \"text\", \"clip_{}_speaker_{}.npy\".format(idx, 1)])\n",
    "        \n",
    "        input_audio_on_screen = np.load(onscreen_audio_feature_path)\n",
    "        input_audio_off_screen = np.load(offscreen_audio_feature_path)\n",
    "        \n",
    "        input_text_on_screen = np.load(onscreen_text_feature_path)\n",
    "        input_text_off_screen = np.load(offscreen_text_feature_path)\n",
    "            \n",
    "        if input_audio_on_screen.shape[0] < input_text_on_screen.shape[0]:\n",
    "            missing_frames = input_text_on_screen.shape[0] - input_audio_on_screen.shape[0]\n",
    "            padding = np.tile(np.expand_dims(self.filler, axis=0), [missing_frames, 1])\n",
    "            input_audio_on_screen = np.concatenate([input_audio_on_screen, padding], axis=0)\n",
    "            input_audio_off_screen = np.concatenate([input_audio_off_screen, padding], axis=0)\n",
    "        input_vector_onscreen = np.concatenate([input_audio_on_screen, input_text_on_screen], axis=1)\n",
    "        input_vector_offscreen = np.concatenate([input_audio_off_screen, input_text_off_screen], axis=1)\n",
    "        input_vector = np.concatenate([input_vector_onscreen, input_vector_offscreen], axis=1)\n",
    "\n",
    "        saliency_map = np.load(os.path.join(self.data_root_path, \"saliency_map\", f\"clip_{idx}.npy\"))\n",
    "\n",
    "        fixations = np.load(os.path.join(self.data_root_path, \"fixation\", f\"clip_{idx}.npy\"))\n",
    "        if len(fixations.shape) == 2:\n",
    "            targets = []\n",
    "            for i in range(fixations.shape[0]):\n",
    "                cur_target = np.zeros(saliency_map.shape)\n",
    "                fixation_center = fixations[i]\n",
    "                image_to_add = increment_pixel_gaussian(cur_target, fixation_center, 10)\n",
    "                assert cur_target.shape == image_to_add.shape\n",
    "                targets.append(image_to_add)\n",
    "        else:\n",
    "            targets = [np.zeros(saliency_map.shape) for _ in range(input_vector.shape[0])]\n",
    "\n",
    "        input_vector = input_vector.reshape(input_vector.shape[0], 1, input_vector.shape[1])\n",
    "        saliency_map = np.array([saliency_map for _ in range(input_vector.shape[0])])\n",
    "        return input_vector, np.expand_dims(saliency_map, 1), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SaliencyMapAudio('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 1, 1674)\n",
      "(250, 1, 288, 472)\n",
      "(250, 288, 472)\n"
     ]
    }
   ],
   "source": [
    "# x1 is the input vector\n",
    "# x2 is the saliency map\n",
    "# y is the target\n",
    "original_x1 = dataset.__getitem__(0)[0]\n",
    "original_x2 = dataset.__getitem__(0)[1]\n",
    "original_y= dataset.__getitem__(0)[2]\n",
    "print(original_x1.shape)\n",
    "print(original_x2.shape)\n",
    "print(original_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28a1f0bb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(original_y[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAudioNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, image_width, image_height):\n",
    "        # torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        super().__init__()\n",
    "        lstm_hidden_size = 512\n",
    "        num_lstm_lyers = 5\n",
    "        self.activation = nn.Sigmoid()\n",
    "        dropout = 0.2\n",
    "        bidirectional = True\n",
    "        kernel_size = 7\n",
    "        upconv_stride = (2, 1)\n",
    "        dilation = (7, 11)\n",
    "        upconv_kernel_size = (6, 11)\n",
    "        output_padding = (3, 3)\n",
    "\n",
    "        # Audio Net\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=lstm_hidden_size, num_layers=num_lstm_lyers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.audio_output1 = [nn.Linear(lstm_hidden_size*2 if bidirectional else lstm_hidden_size, 512), self.activation, nn.Dropout(dropout)]\n",
    "        self.audio_output2 = [nn.Linear(512, 256), self.activation, nn.Dropout(dropout)]\n",
    "        self.audio_output3 = [nn.Linear(256, 128), nn.Sigmoid()]\n",
    "        self.audio_output = nn.Sequential(*self.audio_output1, *self.audio_output2, *self.audio_output3)\n",
    "\n",
    "        # Image Net\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size)\n",
    "        self.conv3 = nn.Conv2d(8, 1, kernel_size)\n",
    "        width = self.compute_conv_image_dim(3, kernel_size, 1, 0, image_width)\n",
    "        height = self.compute_conv_image_dim(3, kernel_size, 1, 0, image_height)\n",
    "        self.image_output1 = [nn.Linear(int(width*height), 512), self.activation, nn.Dropout(dropout)]\n",
    "        self.image_output2 = [nn.Linear(512, 256), self.activation, nn.Dropout(dropout)]\n",
    "        self.image_output3 = [nn.Linear(256, 128), nn.Sigmoid()]\n",
    "        self.image_output = nn.Sequential(*self.image_output1, *self.image_output2, *self.image_output3)\n",
    "\n",
    "        # Upconv net\n",
    "        self.upconv1 = nn.ConvTranspose2d(1, 4, kernel_size=upconv_kernel_size, stride=upconv_stride, dilation=dilation, output_padding=output_padding)\n",
    "        self.upconv2 = nn.ConvTranspose2d(4, 8, kernel_size=upconv_kernel_size, stride=upconv_stride, dilation=dilation, output_padding=output_padding)\n",
    "        self.upconv3 = nn.ConvTranspose2d(8, 1, kernel_size=upconv_kernel_size, stride=upconv_stride, dilation=dilation, output_padding=output_padding)\n",
    "        self.upconv_output = nn.Sequential(self.upconv1, self.upconv2, self.upconv3, nn.Sigmoid())\n",
    "        # W_out = (W_in −1)×stride[1] + dilation[1]×(kernel_size[1]−1) + 1\n",
    "        # image_width = (intermediate.shape[1] - 1) * upconv_stride[1] + dilation[1] * (kernel_size - 1) + 1\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Audio Net\n",
    "        x1_out, x1_hidden = self.lstm(x1)\n",
    "        x1_out = self.audio_output(x1_out)\n",
    "\n",
    "        # Image Net\n",
    "        x2_out = self.conv1(x2)\n",
    "        x2_out = self.conv2(x2_out)\n",
    "        x2_out = self.conv3(x2_out)\n",
    "        x2_out = x2_out.view(x2_out.size(0), -1)\n",
    "        x2_out = self.image_output(x2_out)\n",
    "\n",
    "        if len(x1_out.shape) == 3:\n",
    "            x1_out = x1_out.squeeze()\n",
    "        intermediate_output = torch.vstack((x1_out, x2_out))\n",
    "        # Tensor of size (N, 2, 128)\n",
    "        intermediate_output = intermediate_output.reshape(x1_out.shape[0], 1, 2, 128)\n",
    "\n",
    "        output = self.upconv_output(intermediate_output)\n",
    "        pad = (3, 2, 7, 6) # pad width by 3 on left and 2 on right, pad height by 7 on top and 6 on bottom\n",
    "        output = nn.functional.pad(output, pad)\n",
    "        return output\n",
    "    \n",
    "    def compute_conv_image_dim(self, num_convs, kernel_size, stride, padding, input_dim):\n",
    "        for _ in range(num_convs):\n",
    "            input_dim = (input_dim - kernel_size + 2*padding)/stride + 1\n",
    "        return input_dim\n",
    "    \n",
    "    def compute_upconv_image_dim(self, num_convs, kernel_size, stride, padding, input_dim):\n",
    "        for _ in range(num_convs):\n",
    "            input_dim = (input_dim - 1)*stride - 2*padding + kernel_size - 1\n",
    "        return input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 288, 472])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ImageAudioNet(original_x1.shape[2], original_x2.shape[3], original_x2.shape[2])\n",
    "model(torch.tensor(original_x1)[0:5].to(torch.float32), torch.tensor(original_x2)[0:5].to(torch.float32)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "model.to(torch.float32)\n",
    "model.to(device)\n",
    "model.train() \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "training_loss = []\n",
    "\n",
    "for epoch in trange(1, epochs + 1):\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for i in trange(dataset.__len__()):\n",
    "        x1, x2, y = dataset.__getitem__(i)\n",
    "        x1, x2, y = torch.tensor(x1).to(torch.float32), torch.tensor(x2).to(torch.float32), torch.tensor(y).to(torch.float32)\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "        pred = model(x1, x2)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_train_loss += loss.item()\n",
    "        del x1, x2, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    training_loss.append(total_train_loss/len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Fixed Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from datetime import datetime\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'output'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "use_wandb = False\n",
    "image_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to increment image values in a gaussian style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_pixel_gaussian(image, center, sigma):\n",
    "    # Create a 2D Gaussian kernel\n",
    "    kernel_size = int(2 * np.ceil(2 * sigma) + 1)\n",
    "    kernel = np.zeros((kernel_size, kernel_size))\n",
    "    center_index = int(kernel_size / 2)\n",
    "    for i in range(kernel_size):\n",
    "        for j in range(kernel_size):\n",
    "            x = i - center_index\n",
    "            y = j - center_index\n",
    "            kernel[i, j] = np.exp(-(x ** 2 + y ** 2) / (2 * sigma ** 2))\n",
    "    kernel /= np.sum(kernel)\n",
    "    \n",
    "    # Increment the pixel values according to the Gaussian kernel\n",
    "    row, col = int(center[0]), int(center[1])\n",
    "    row_min = int(max(row - center_index, 0))\n",
    "    row_max = int(min(row + center_index + 1, image.shape[0]))\n",
    "    col_min = int(max(col - center_index, 0))\n",
    "    col_max = int(min(col + center_index + 1, image.shape[1]))\n",
    "    window = image[row_min:row_max, col_min:col_max]\n",
    "    weights = kernel[center_index - (row - row_min):center_index + (row_max - row), \n",
    "                     center_index - (col - col_min):center_index + (col_max - col)]\n",
    "    window += weights\n",
    "    image[row_min:row_max, col_min:col_max] = window\n",
    "    return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_pixel_gaussian(image, center, sigma):\n",
    "    # Create a 2D Gaussian kernel\n",
    "    kernel_size = int(2 * np.ceil(2 * sigma) + 1)\n",
    "    kernel = np.zeros((kernel_size, kernel_size))\n",
    "    center_index = int(kernel_size / 2)\n",
    "    for i in range(kernel_size):\n",
    "        for j in range(kernel_size):\n",
    "            x = i - center_index\n",
    "            y = j - center_index\n",
    "            kernel[i, j] = np.exp(-(x ** 2 + y ** 2) / (2 * sigma ** 2))\n",
    "    kernel /= np.sum(kernel)\n",
    "    \n",
    "    # Increment the pixel values according to the Gaussian kernel\n",
    "    row, col = int(center[0]), int(center[1])\n",
    "    row_min = int(max(row - center_index, 0))\n",
    "    row_max = int(min(row + center_index + 1, image.shape[0]))\n",
    "    col_min = int(max(col - center_index, 0))\n",
    "    col_max = int(min(col + center_index + 1, image.shape[1]))\n",
    "    window = image[row_min:row_max, col_min:col_max]\n",
    "    weights = kernel[center_index - (row - row_min):center_index + (row_max - row), \n",
    "                     center_index - (col - col_min):center_index + (col_max - col)]\n",
    "    window += weights\n",
    "    image[row_min:row_max, col_min:col_max] = window\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyMapAudio(Dataset):\n",
    "    def __init__(self, processed_data_path, image_output):\n",
    "        self.filler = np.array([-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715])\n",
    "        # save dataset root path\n",
    "        self.data_root_path = processed_data_path\n",
    "        # load video names\n",
    "        video_names_path = os.path.join(*[self.data_root_path, \"video_to_window_metadata.json\"])\n",
    "        self.metadata = json.load(open(video_names_path, \"r\"))\n",
    "        self.all_files_in_set = []\n",
    "        self.image_output = image_output\n",
    "        videos_included = list(self.metadata.keys())\n",
    "        for i in videos_included:\n",
    "            self.all_files_in_set = self.all_files_in_set + self.metadata[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files_in_set)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        onscreen_audio_feature_path = os.path.join(*[self.data_root_path, \"audio\", \"clip_{}_speaker_{}.npy\".format(idx, 0)])\n",
    "        offscreen_audio_feature_path = os.path.join(*[self.data_root_path, \"audio\", \"clip_{}_speaker_{}.npy\".format(idx, 1)])\n",
    "        onscreen_text_feature_path = os.path.join(*[self.data_root_path, \"text\", \"clip_{}_speaker_{}.npy\".format(idx, 0)])\n",
    "        offscreen_text_feature_path = os.path.join(*[self.data_root_path, \"text\", \"clip_{}_speaker_{}.npy\".format(idx, 1)])\n",
    "        \n",
    "        input_audio_on_screen = np.load(onscreen_audio_feature_path)\n",
    "        input_audio_off_screen = np.load(offscreen_audio_feature_path)\n",
    "        \n",
    "        input_text_on_screen = np.load(onscreen_text_feature_path)\n",
    "        input_text_off_screen = np.load(offscreen_text_feature_path)\n",
    "            \n",
    "        if input_audio_on_screen.shape[0] < input_text_on_screen.shape[0]:\n",
    "            missing_frames = input_text_on_screen.shape[0] - input_audio_on_screen.shape[0]\n",
    "            padding = np.tile(np.expand_dims(self.filler, axis=0), [missing_frames, 1])\n",
    "            input_audio_on_screen = np.concatenate([input_audio_on_screen, padding], axis=0)\n",
    "            input_audio_off_screen = np.concatenate([input_audio_off_screen, padding], axis=0)\n",
    "        input_vector_onscreen = np.concatenate([input_audio_on_screen, input_text_on_screen], axis=1)\n",
    "        input_vector_offscreen = np.concatenate([input_audio_off_screen, input_text_off_screen], axis=1)\n",
    "        input_vector = np.concatenate([input_vector_onscreen, input_vector_offscreen], axis=1)\n",
    "\n",
    "        saliency_map = np.load(os.path.join(self.data_root_path, \"saliency_map\", f\"clip_{idx}.npy\"))\n",
    "        # Already between -180 and 180 shifted by 180 due to pre-processing\n",
    "        fixations = np.load(os.path.join(self.data_root_path, \"fixation\", f\"clip_{idx}.npy\"))\n",
    "        if self.image_output:\n",
    "            if len(fixations.shape) == 2:\n",
    "                targets = []\n",
    "                for i in range(fixations.shape[0]):\n",
    "                    cur_target = np.zeros(saliency_map.shape)\n",
    "                    fixation_center = fixations[i]\n",
    "                    image_to_add = increment_pixel_gaussian(cur_target, fixation_center, 10)\n",
    "                    assert cur_target.shape == image_to_add.shape\n",
    "                    targets.append(image_to_add)\n",
    "            else:\n",
    "                targets = [np.zeros(saliency_map.shape) for _ in range(input_vector.shape[0])]\n",
    "        else:\n",
    "            if len(fixations.shape) == 2:\n",
    "                targets = fixations.copy() / 360\n",
    "            else:\n",
    "                targets = [np.ones(fixations.shape)*0.5 for _ in range(input_vector.shape[0])]\n",
    "\n",
    "        input_vector = input_vector.reshape(input_vector.shape[0], 1, input_vector.shape[1])\n",
    "        saliency_map = np.array([saliency_map for _ in range(input_vector.shape[0])])\n",
    "        # The targets are shifted by 180 degrees due to pre-processing\n",
    "        # To get actual values, just subtract 180 from all x,y rotations\n",
    "        return input_vector, np.expand_dims(saliency_map, 1), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SaliencyMapAudio('output', image_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 is the input vector\n",
    "# x2 is the saliency map\n",
    "# y is the target\n",
    "original_x1 = dataset.__getitem__(0)[0]\n",
    "original_x2 = dataset.__getitem__(0)[1]\n",
    "original_y= dataset.__getitem__(0)[2]\n",
    "print(original_x1.shape)\n",
    "print(original_x2.shape)\n",
    "print(original_y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model predicts either an image or x,y rotations between 0 and 1.\n",
    "To convert to actual rotations, multiply by 360 and then subtract 180 since inputs are shifted by 180.\n",
    "The result will be between -180 and 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAudioNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, image_width, image_height, output_image):\n",
    "        # torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        super().__init__()\n",
    "        lstm_hidden_size = 512\n",
    "        num_lstm_lyers = 5\n",
    "        self.activation = nn.Sigmoid()\n",
    "        dropout = 0.2\n",
    "        bidirectional = True\n",
    "        kernel_size = 7\n",
    "        upconv_stride = (2, 1)\n",
    "        dilation = (8, 12)\n",
    "        upconv_kernel_size = (7, 7)\n",
    "        output_padding = (2, 5)\n",
    "        self.output_image = output_image\n",
    "\n",
    "        # Audio Net\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=lstm_hidden_size, num_layers=num_lstm_lyers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.audio_output1 = [nn.Linear(lstm_hidden_size*2 if bidirectional else lstm_hidden_size, 512), self.activation, nn.Dropout(dropout)]\n",
    "        self.audio_output2 = [nn.Linear(512, 256), self.activation, nn.Dropout(dropout)]\n",
    "        self.audio_output3 = [nn.Linear(256, 128), nn.Sigmoid()]\n",
    "        self.audio_output = nn.Sequential(*self.audio_output1, *self.audio_output2, *self.audio_output3)\n",
    "\n",
    "        # Image Net\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size)\n",
    "        self.conv3 = nn.Conv2d(8, 1, kernel_size)\n",
    "        width = self.compute_conv_image_dim(3, kernel_size, 1, 0, image_width)\n",
    "        height = self.compute_conv_image_dim(3, kernel_size, 1, 0, image_height)\n",
    "        self.image_output1 = [nn.Linear(int(width*height), 512), self.activation, nn.Dropout(dropout)]\n",
    "        self.image_output2 = [nn.Linear(512, 256), self.activation, nn.Dropout(dropout)]\n",
    "        self.image_output3 = [nn.Linear(256, 128), nn.Sigmoid()]\n",
    "        self.image_output = nn.Sequential(*self.image_output1, *self.image_output2, *self.image_output3)\n",
    "\n",
    "        # Upconv net\n",
    "        self.upconv1 = nn.ConvTranspose2d(1, 4, kernel_size=upconv_kernel_size, stride=upconv_stride, dilation=dilation, output_padding=output_padding)\n",
    "        self.upconv2 = nn.ConvTranspose2d(4, 8, kernel_size=upconv_kernel_size, stride=upconv_stride, dilation=dilation, output_padding=output_padding)\n",
    "        self.upconv3 = nn.ConvTranspose2d(8, 1, kernel_size=upconv_kernel_size, stride=upconv_stride, dilation=dilation, output_padding=output_padding)\n",
    "        self.upconv_output = nn.Sequential(self.upconv1, self.upconv2, self.upconv3, nn.Sigmoid())\n",
    "        \n",
    "        # Classification net\n",
    "        self.classification1 = [nn.Linear(256, 128), self.activation, nn.Dropout(dropout)]\n",
    "        self.classification2 = [nn.Linear(128, 64), self.activation, nn.Dropout(dropout)]\n",
    "        self.classification3 = [nn.Linear(64, 16), self.activation, nn.Dropout(dropout)]\n",
    "        self.classification4 = [nn.Linear(16, 2), nn.Sigmoid()]\n",
    "        self.classification_output = nn.Sequential(*self.classification1, *self.classification2, *self.classification3, *self.classification4)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Audio Net\n",
    "        x1_out, x1_hidden = self.lstm(x1)\n",
    "        x1_out = self.audio_output(x1_out)\n",
    "\n",
    "        # Image Net\n",
    "        x2_out = self.conv1(x2)\n",
    "        x2_out = self.conv2(x2_out)\n",
    "        x2_out = self.conv3(x2_out)\n",
    "        x2_out = x2_out.view(x2_out.size(0), -1)\n",
    "        x2_out = self.image_output(x2_out)\n",
    "\n",
    "        if len(x1_out.shape) == 3:\n",
    "            x1_out = x1_out.squeeze()\n",
    "            # Tensor of size (N, 2, 128)\n",
    "        intermediate_output = torch.vstack((x1_out, x2_out))\n",
    "        \n",
    "        if self.output_image:\n",
    "            intermediate_output = intermediate_output.reshape(x1_out.shape[0], 1, 2, 128)\n",
    "            output = self.upconv_output(intermediate_output)\n",
    "            pad = (1, 0, 1, 0) # pad width by 1 on left and 0 on right, pad height by 1 on top and 0 on bottom\n",
    "            output = nn.functional.pad(output, pad)\n",
    "        else:\n",
    "            intermediate_output = intermediate_output.reshape(x1_out.shape[0], 256)\n",
    "            output = self.classification_output(intermediate_output)\n",
    "        return output\n",
    "    \n",
    "    def compute_conv_image_dim(self, num_convs, kernel_size, stride, padding, input_dim):\n",
    "        for _ in range(num_convs):\n",
    "            input_dim = (input_dim - kernel_size + 2*padding)/stride + 1\n",
    "        return input_dim\n",
    "    \n",
    "    def compute_upconv_image_dim(self, num_convs, kernel_size, stride, padding, input_dim):\n",
    "        for _ in range(num_convs):\n",
    "            input_dim = (input_dim - 1)*stride - 2*padding + kernel_size - 1\n",
    "        return input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageAudioNet(original_x1.shape[2], original_x2.shape[3], original_x2.shape[2], image_output)\n",
    "model(torch.tensor(original_x1)[0:5].to(torch.float32), torch.tensor(original_x2)[0:5].to(torch.float32)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotations_to_saliency_map(x, y):\n",
    "    # Convert x,y rotations to Saliency Map\n",
    "    image = np.zeros((360, 360))\n",
    "    return increment_pixel_gaussian(image, (x, y), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_rotations_to_original(x, y, positive_only=True):\n",
    "    # Shift rotations to original\n",
    "    x -= 180\n",
    "    y -= 180\n",
    "    if not positive_only:\n",
    "        return x, y\n",
    "    if x < 0:\n",
    "        x += 360\n",
    "    if y < 0:\n",
    "        y += 360\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = torch.utils.data.random_split(dataset, [0.7, 0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_wandb:\n",
    "    wandb.login()\n",
    "    run_obj = wandb.init(project=\"gazeEstimationSaliencyMap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = StepLR(optimiser, step_size=20, gamma=0.00000000001)\n",
    "model.to(torch.float32)\n",
    "model.to(device)\n",
    "model.train() \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "count = 0\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.watch(model, log='all')\n",
    "\n",
    "for epoch in trange(1, epochs + 1):\n",
    "    # Training\n",
    "    total_train_loss = 0\n",
    "    for i in trange(train.__len__()):\n",
    "        x1, x2, y = dataset.__getitem__(i)\n",
    "        x1, x2, y = torch.tensor(x1).to(torch.float32), torch.tensor(x2).to(torch.float32), torch.tensor(y).to(torch.float32)\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "        pred = model(x1, x2)\n",
    "        del x1, x2\n",
    "        y = y.to(device)\n",
    "        pred, y = pred.reshape(pred.shape[0], torch.prod(torch.tensor(pred.shape[1:]))), y.reshape(y.shape[0], torch.prod(torch.tensor(y.shape[1:])))\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_train_loss += loss.item()\n",
    "        del y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    total_valid_loss = 0\n",
    "    for i in trange(valid.__len__()):\n",
    "        x1, x2, y = dataset.__getitem__(i)\n",
    "        x1, x2, y = torch.tensor(x1).to(torch.float32), torch.tensor(x2).to(torch.float32), torch.tensor(y).to(torch.float32)\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "        pred = model(x1, x2)\n",
    "        del x1, x2\n",
    "        y = y.to(device)\n",
    "        pred, y = pred.reshape(pred.shape[0], torch.prod(torch.tensor(pred.shape[1:]))), y.reshape(y.shape[0], torch.prod(torch.tensor(y.shape[1:])))\n",
    "        loss = loss_fn(pred, y)\n",
    "        total_valid_loss += loss.item()\n",
    "        del y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    validation_loss.append(total_valid_loss/len(dataset))\n",
    "    training_loss.append(total_train_loss/len(dataset))\n",
    "    if use_wandb:\n",
    "        wandb.log({'training loss': total_train_loss, 'validation loss': total_valid_loss})\n",
    "\n",
    "\n",
    "    file_name = f'time={datetime.now()}_epoch={epoch}.pt'\n",
    "    save_path = os.path.join('models', file_name)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    if use_wandb:\n",
    "        wandb.save(file_name)\n",
    "\n",
    "    # Early Stopping\n",
    "    if epoch > 1:\n",
    "        if total_valid_loss > validation_loss[epoch - 2]:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "    if count == 5:\n",
    "        print('\\n\\nStopping early due to decrease in performance on validation set\\n\\n')\n",
    "        break   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test in tqdm(train):\n",
    "#     x1, x2, y = test\n",
    "#     x1, x2, y = torch.tensor(x1).to(torch.float32), torch.tensor(x2).to(torch.float32), torch.tensor(y).to(torch.float32)\n",
    "#     assert torch.max(y) <= 1\n",
    "#     assert torch.max(x2) <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
